{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/batistajunior/anaconda3/lib/python3.10/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Bem-vindo ao meu aplicativo Flask no Heroku!'\n",
    "\n",
    "@app.route('/analise')\n",
    "def analise():\n",
    "    # Etapa 1: Carregamento dos dados\n",
    "    df_sensor = pd.read_csv('https://raw.githubusercontent.com/Batistajunior/desafio-de-dados-fieldpro/main/Sensor_FieldPRO.csv')\n",
    "    df_estacao = pd.read_csv('https://raw.githubusercontent.com/Batistajunior/desafio-de-dados-fieldpro/main/Estacao_Convencional.csv')\n",
    "\n",
    "    # Etapa 2: Verificação dos dados\n",
    "    print(\"Dados do Sensor:\")\n",
    "    display(df_sensor.head())\n",
    "    print(\"Dados da Estação:\")\n",
    "    display(df_estacao.head())\n",
    "\n",
    "    # Etapa 3: Pré-processamento dos dados\n",
    "    df_sensor['Datetime – utc'] = pd.to_datetime(df_sensor['Datetime – utc'], format='ISO8601', utc=True)\n",
    "    df_sensor['data'] = df_sensor['Datetime – utc'].dt.date\n",
    "    df_sensor['Hora (Brasília)'] = df_sensor['Datetime – utc'].dt.time\n",
    "    df_sensor['data'] = pd.to_datetime(df_sensor['data'])\n",
    "    df_sensor['Hora (Brasília)'] = pd.to_datetime(df_sensor['Hora (Brasília)'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "    df_sensor = df_sensor.drop(columns=['Datetime – utc'])\n",
    "\n",
    "    # Reordenando as colunas com 'data' e 'Hora (Brasília)' na frente\n",
    "    df_sensor = df_sensor[['data', 'Hora (Brasília)', 'air_humidity_100', 'air_temperature_100', 'atm_pressure_main', 'num_of_resets', 'piezo_charge', 'piezo_temperature']]\n",
    "\n",
    "    # Convertendo as colunas 'data' e 'Hora (Brasília)' para o tipo datetime no DataFrame df_estacao\n",
    "    df_estacao['data'] = pd.to_datetime(df_estacao['data'])\n",
    "    df_estacao['Hora (Brasília)'] = pd.to_datetime(df_estacao['Hora (Brasília)'], format='%H:%M:%S', errors='coerce').dt.time\n",
    "\n",
    "    # Juntando os DataFrames df_sensor e df_estacao com base nas colunas 'data' e 'Hora (Brasília)'\n",
    "    df_completo = pd.merge(df_sensor, df_estacao, on=['data', 'Hora (Brasília)'], how='inner')\n",
    "\n",
    "    # Etapa 4: Análise exploratória dos dados e visualização de gráficos\n",
    "    # Verificando quais colunas têm dados válidos para plotar os gráficos\n",
    "    valid_columns = ['air_humidity_100', 'air_temperature_100', 'atm_pressure_main', 'num_of_resets', 'piezo_charge', 'piezo_temperature', 'chuva']\n",
    "\n",
    "    # Verificando se há alguma coluna com dados válidos para plotar os gráficos\n",
    "    if all(df_completo[col].notnull().any() for col in valid_columns):\n",
    "        # Visualizando a distribuição das variáveis e plotando os gráficos\n",
    "        fig, axes = plt.subplots(1, 7, figsize=(24, 5))\n",
    "        for i, col in enumerate(valid_columns):\n",
    "            sns.histplot(df_completo[col], bins=20, edgecolor='black', color='skyblue', alpha=0.8, ax=axes[i])\n",
    "            axes[i].set_title(col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Verificando a correlação entre as variáveis numéricas\n",
    "        correlation_matrix = df_completo[valid_columns].corr()\n",
    "\n",
    "        # Plotando a matriz de correlação\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=1, linecolor='black', annot_kws={\"size\": 10})\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.title('Matriz de Correlação', fontsize=14)\n",
    "        plt.show()\n",
    "\n",
    "    # Etapa 5: Preparação dos dados para treinamento do modelo\n",
    "    # Definindo as features e o target\n",
    "    features = ['air_humidity_100', 'air_temperature_100', 'atm_pressure_main', 'num_of_resets', 'piezo_charge', 'piezo_temperature']\n",
    "    target = 'chuva'\n",
    "\n",
    "    # Removendo amostras com valores ausentes\n",
    "    df_completo.dropna(subset=features + [target], inplace=True)\n",
    "\n",
    "    # Separando os dados em features (X) e target (y)\n",
    "    X = df_completo[features]\n",
    "    y = df_completo[target]\n",
    "\n",
    "    # Dividindo os dados em conjuntos de treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Preenchendo valores ausentes com a média\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_filled = imputer.fit_transform(X_train)\n",
    "    X_test_filled = imputer.transform(X_test)\n",
    "\n",
    "    # Etapa 6: Treinamento e avaliação do modelo\n",
    "    # Criando e treinando o modelo de regressão com HistGradientBoostingRegressor\n",
    "    model = HistGradientBoostingRegressor()\n",
    "    model.fit(X_train_filled, y_train)\n",
    "\n",
    "    # Realizando as previsões no conjunto de teste\n",
    "    y_pred = model.predict(X_test_filled)\n",
    "\n",
    "    # Avaliando o desempenho do modelo\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Erro Médio Absoluto (MAE):\", mae)\n",
    "    print(\"Erro Quadrático Médio (MSE):\", mse)\n",
    "    print(\"Coeficiente de Determinação (R²):\", r2)\n",
    "\n",
    "    # Etapa 7: Salvando o modelo treinado\n",
    "    # Salvando o modelo em um arquivo joblib\n",
    "    modelo_treinado_path = \"modelo_treinado.joblib\"\n",
    "    joblib.dump(model, modelo_treinado_path)\n",
    "\n",
    "    return f\"Erro Médio Absoluto (MAE): {mae}<br>Erro Quadrático Médio (MSE): {mse}<br>Coeficiente de Determinação (R²): {r2}\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
